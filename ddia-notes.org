#+title: DDIA Notes
#+author: Evan Black

* Part 1: Foundations of Data Systems
** Chapter 1: Reliable, Scalable, and Maintainable Applications

There are common building blocks of applications we can reuse, and should.

- Database: Generic storage and retrieval
- Cache: Speed up expensive operations by remembering results
- Search Index: Allow users to search using keywords and filters
- Stream Processing: Async message sending and receiving
- Batch Processing: Periodic crunching of accumulated data

We should look at the characteristics of these things and our requirements to determine the best option for our use-case.

*** Thinking About Data Systems

Increasing number of tools may seem similar from a high level, but have different strengths.
e.g. Postgres and Redis both "store data", but they have very different use cases in mind.

Nowadays, we stitch apps together with multiple tools, trying to leverage the best of each. The app's API will hide these implementation details.

But now that data flows through all of these data systems, how can we make sure that things stay working in the face of errors in one of them? How can we keep things consistent?

This book focuses on three concerns:

- Reliability: Working correctly in the face of adversity.
- Scalability: Dealing with growth in volume.
- Maintainability: Keeping the top-level app and underlying code clean and understandable.

*** Reliability

Generally, reliability is about the app operating correctly even when encountering hardware, software, or user faults.

A fault and a failure are different. Faults are smaller in scale, whereas a failure is complete. Fault-tolerance is the name of the game, to prevent faults from becoming failures.

To counter hardware faults, you can add redundancy at the component level, e.g. multiple hard-drives, or at the machine level, e.g. AFS / cloud computing.

Software faults are generally harder to fix, but testing things in isolation and monitoring guarantees of the system can go a long way.

Human errors can be countered by applying multiple practices: well-designed abstractions / APIs, sandbox environments, layers of testing, easy data recovery, user telemetry, and training.

Reliability is important, but it must be understood in the context of the PPP'S purpose. Software for a busy online store would require a greater focus on reliability than a free app for making funny GIFs.

*** Scalability

When talking about scalability, speak with specific numbers in mind. You can't just say an app is or isn't scalable.

One way to discuss is to pick a few key activities that induce "load", and then work in multiples of that data. If the app can handle X requests per second, then you can ask "What would it take to handle 5 times as many requests per second?"

Twitter ran into trouble scaling out their app. Originally they implemented a global tweet table some, but then they switched to a fan-out messaging approach. Eventually, they settled on a hybrid approach, fan-out for small-time users, and global lookup for messages from popular users.

Once able to describe the current load and some future load, you can ask questions like:

- If we didn't scale up resources, how would things be affected?
- If we wanted to offer the same response times, how many more resources would it take?

In batch systems, we should care more about throughput. In user-facing systems, however, we should care more about response time. There will be great variance in either of them, so we need to speak statistically about their performance.

For response times, instead of the average, you'll want to look at things like p95 or p99, which are like common worst-cases. True one-in-a-million outliers will likely not be worth optimizing for, though.

(You should check out wrk2, which is a "constant throughput, correct latency recording" HTTP load generator.)

To solve these scaling issues, you can scale up (bigger machine) or scale out (more machines). Usually the best solution is a pragmatic combination of both techniques.

*** Maintainability

Here are three design principles to keep in mind when doing maintenance:

- Operability: Making it easy for the Ops team
- Simplicity: Making it easy for new engineers to understand
- Evolvability: Making it easy for engineers to make changes

For operability, we can: add health monitoring to the app, make it easy to update, have useful error messages, and write good documentation.

For simplicity, we can try to reduce complexity within the code by using good abstractions.

To maintain good evolvability, you'll likely reach for a lot of the same things mentioned prior. This also involves organizational change, though, with placing greater emphasis on writing tests and implementing agile processes.

** Chapter 2: Data Models and Query Languages

How we model our data informs how we approach solving our problems. Apps are generally composed of layers of data representations, e.g. the difference between an object in JS, a row in Postgres, and data on a disk.

This chapter focuses on the different data models, and how it affects our representation and ability to query data.

*** Relational Model Versus Document Model

Originally, people were unsure if relational databases could be implemented efficiently. Competitors have come and gone, while the relational model has stayed with us.

NoSQL (retroactively changed to mean "Not Only SQL") is yet another model. They focus on offering greater scalability and expressiveness compared to existing implementations of the relational model.

In OOP languages, there is an impedance mismatch that occurs between how data is stored and retrieved from a relational database.

The relational model is a good fit when many disparate entries will share unified data (many-to-one), whereas the document model will work better in situations where data is not semantically shared between entries (one-to-many).

NoSQL's document model seems somewhat similar to the out-moded hierarchical model. Dissatisfaction with this old model gave rise to the relational model and the network model.

The network model was also used for a time, but difficulties in querying and modifying data lead to its disuse. The relational model avoided these issues by leaving the "how" of its traversal to a query optimizer, the presence of which allows for this problem to be solved more generically than the network model.

There are certain advantages to NoSQL's document model if your application data allows for it. Storing disconnected, nested data is simpler and the lack of a schema may make it easier to just roll-forward with data changes at the application level. Since multiple levels may be stored under a single document, you can avoid performance penalties from doing a join, as you would need to with SQL.

With the introduction of things like XML and JSON values to Postgres, you may be able to reap the benefits of both models from one database.

*** Query Languages for Data

SQL is declarative, while the query language for IMS and CODASYL were imperative. This allows for the query optimizer to do its job. With SQL, you describe the "what" and the database figures out the "how".

CSS is also declarative. Things would not be fun if it were imperative.

MapReduce, which is a technique often utilized with NoSQL datastores, is a mix of both declarative and imperative. The imperative bits are restricted by being required to be pure; they can only operate on the data given to them.

While there is great power in being able to write arbitrary javascript in a MapReduce function, it makes optimization difficult. MongoDB introduced a declarative "aggregation pipeline" feature to help with this, which uses JSON instead of something that looks like an english sentence. This feels a lot like SQL, still...

*** Graph-Like Data Models

The relational model gets unwieldy as things start to look more like a graph than a tree. Graph databases use vertices (a.k.a. nodes or entities) and edges (a.k.a. relationships or arcs) to represent graph-like data.

There are two models for graph databases: property graph and triple store. The property graph model can offer great flexibility, as there is no schema enforcing the things that can be connected. It can store multiple properties on both vertices and edges. The triple store is similar, but much simpler, using declarations of (subject, predicate, object) to construct the graph. Something like the list of properties would have to be stored through a series of connections rather than on a vertex or edge directly.

Cypher, SPARQL, and Datalog are some examples of graph query languages.

(I've gone a bit light on this section, because it feels a bit too specific for this section of the book, and of limited worth when compressed.)

*** Summary

You can see the document model and the graph model as two opposite sides. The document model excels when related data can be contained within a single document, whereas the graph model does well at making and traversing connections between separately stored things easy.

** Chapter 3: Storage and Retrieval

While the previous chapter presented an app developer's perspective on storing and retrieving data using a database, this chapter is about how the database itself goes about storing and retrieving data. Knowing how a database stores and retrieves data will pay dividends when you inevitably have to tune and manage it.

*** Data Structures That Power Your Database
**** Log-Structued Storage Engines

Log-structured storage engines are a common type for databases. They simply append new data over time, using the most recently appended version. While it is fast for inserts, it is slow for reads, as you have to scan the file to find the most recent entry.
To get around this, log-structured storage engines will use an index, which is a separate lookup structure used to speed up reads.

One implementation of an index is a hash index. This hashes each of the entries by key along with an offset. This makes lookups much faster.

Eventually, the data file may be split into multiple smaller files as time goes on. At some point, they may also be merged in a process called compaction, where only the most recent, non-deleted entries will be kept.

Log-structured storage engines have a few benefits. Namely, writes are faster than writing in the middle of a file, compaction helps deal with fragmentation, and crash recovery can be simpler since you can just "play back" the changes. However, in order to be fast, the hash table must fit in memory.

**** SSTables and LSM-Trees

SSTables (Sorted String Tables) present an improvement over basic log-structured storage. While they must make additional effort to keep entries ordered upon insertion, things can be much more quickly compacted due to already being sorted. This sorting also eliminates the need for for a full index, since the entries themselves can just be binary-searched over using a sparse index.

The general flow for an SSTable is:

- When a write comes in, add to your in-memory balanced tree
- When the in-memory tree gets too big, write out the whole thing to a file. New writes go to a new memtable.
- To handle a read, try to find it in the current in-memory table, then look into previous segments as needed.
- In the background, merge and compact the previous segment files.

SSTables can suffer from data loss in the case of a crash, but just having a separate log with the most recent unserviced writes in the order received would cover you.

The algorithm described in the previous bit is called an LSM-tree (Log-Structued Merge-Tree).

There are many other things done to improve performance of LSM-trees, like using a bloom filter to more quickly determine if a key does not exist. Multiple approaches exist for determining how and when the SSTables are merged and compacted. In general, LSM-trees are very useful because they offer good performance for both reads and writes.

**** B-Trees

B-trees are much older than SSTables and LSM-trees, becoming "ubiquitous" by the 1980s. They, like SSTables, use sorted key-value pairs. B-trees, however, use fixed-size blocks (also called pages, usually 4 KB in size) instead of the append-only log-like files. These pages are what are read or written to one at a time as needed. They form a tree-like structure, with top-level pages pointing to lower-level ones (called leaf pages). The number of child pages per level is called the "branching factor", with real life DBs having ~500.

B-trees help keep things balanced, by splitting pages as needed. This helps B-trees of with ~n~ entries keep a depth of ~O(log n)~.

"A four level tree of 4 KB pages with a branching factor of 500 can store up to 256 TB."

B-trees, unlike LSM-trees, modify these pages in the middle, and writes may involve changes to multiple levels within the tree. To keep things safe, a WAL (write-ahead log) is kept, and can be replayed in the case of a crash.

Over time, B-trees have accrued additional optimizations, like copy-on-write schemes and keeping track of additional pointers between siblings.

**** Comparing B-Trees and LSM-Trees

It is difficult to give a win to either, as it is dependent on application requirements and the hardware in use. For best results, you'll likely need to do some experimentation.

**** Other Indexing Structures

Many databases support secondary indexes, which allow quicker reads over things not already covered by the primary index.

The value associated to a key may be the row itself, or it could be a pointer to the row on the heap. The heap is useful when multiple secondary indexes are present, since they can all point to the same data instead of duplicating it.

A covering index is a compromise between a clustered index (all row data in the index) and a non-clustered index (only references to the heap within the index).

Multi-column indices are useful and have multiple implementations, like a concatenated index, or an R-tree.

Fuzzy indexing allows for queries for similar words or close matches instead of exact ones.

As RAM grows cheaper, in-memory databases can provide better performance than traditional databases, since they don't have to spend time encoding data to files.

*** Transaction Processing or Analytics?

There are two main, separate uses for databases: transaction processing or analytics.

Transaction processing (also called OLTP (online transaction processing)) is generally concerned with creating, reading, and updating individual rows within the database. Things occurring within a transaction-processing context are generally in response to user actions, and are expected to be handled quickly.

Analytics (also called OLAP (online analytics processing)) is more focused on the aggregation of data, and may be performed over an entire history of events, rather than a current state of things. This data is typically much larger, and worked on in bulk or through streams.

Data-warehousing is the practice of shipping transactional data into a larger OLAP system. This allows a business to find actionable insights without bringing down OLTP systems with their queries.

OLTP systems often have a star or snowflake pattern for their relations, where narrow tables are joined together to create wider ones as needed. OLAP systems may have tables that are hundreds of columns wide, with lots of repetition.

*** Column-Oriented Storage

Since most queries into these wide tables may only be looking to retrieve just a few columns, a column-oriented DB flips the script and stores whole columns together instead of rows. This makes performing aggregations on these columns much faster. Because of repetition, they compress pretty well, too.

Sorting is advantageous for querying and compression, as long as sorting keys are chosen appropriately. e.g. Date and product ID for retail sales data.

LSM-trees make updates to these column-oriented DBs possible.

*** Summary
** Chapter 4: Encoding and Evolution

When working with large-scale applications, we often need to maintain backwards and forwards compatibility of our data, so differing versions of the application code can coexist while changes are rolled out.

*** Formats for Encoding Data

While many programming languages may have their own specific encoding format, it is often better and safer to use standardize, language-agnostic formats.

While JSON, XML, and CSV are pretty ubiquitous and easy to use, they have several downsides, generally around imprecise representations of certain data types like big numbers and dates, and some lack common means of enforcing a schema.

Binary versions of JSON can be useful for cutting down on transmission sizes, but still lack enforcement of schema.

Thrift and Protocol Buffers offer a schema-dependent solution. They have a common file format for expressing schemas, as well as code generation tools to make it easy to integrate with. Because of this schema focus, you must pay careful attention to backwards and forwards compatibility when modifying the schema over time.

Avro uses an even more compact representation, removing fields entirely from the transmission itself. It also has rules for dealing with mismatched schemas between writer and reader, allowing things to just work in some situations.

(Compared to the other two, Avro seems pretty neat! Still feels like we're getting a bit bogged down in the minutiae of these different encoding formats, considering the relative age of the things.)

In general, since we have tools that make schema enforcement easy without making schema evolution too difficult, we should look to utilize those tools instead of just saying "JSON is probably good enough".

*** Models of Dataflow

Databases may apply schema modifications to the underlying data as needed when read, rather than across the board when it went into affect. e.g. Addition of a new column to schema, null is added when read.

Archived data presents another challenge, as the schema stays the same as when it was originally archived. Reading it back to apply to a current schema can be tricky, but things like Avro can make it simpler.

Web APIs should implement similar functionality to something like Avro, but at their larger scale, supporting a range of client ages as the surface of the API changes.

REST (REpresentational State Transfer) as a design philosophy offers several advantages over the complexity of SOAP and the falsehoods of RPC-like (Remote Procedure Call) structure. It doesn't hide the fact that it is a network protocol. While it is generally pretty good for public APIs, The overhead associated with REST may make it less appealing for internal messaging, where something closer to RPCs may be desirable.

Message brokers present another means of transferring data, but generally fills a different use-case than REST or RPC. Generally these systems revolve around brokers, queues, and consumers.

Distributed actor frameworks present an opportunity to make RPCs transparent by creating a system of message-passing between "actors". Things can then be scaled out on a single or multiple machines with little to no changes to the application code, as the messaging model covers both scenarios.
